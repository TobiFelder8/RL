{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccbf74e-1fcb-40c2-98d7-c30d0f73f252",
   "metadata": {},
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "In this exercise we will build a simple deep Q-learning agent from scratch. For this we need to look at the following tasks:\n",
    "* How do we specify the model?\n",
    "* How do we calculate an action?\n",
    "* How do we sample episodes?\n",
    "* How do we train the model?\n",
    "\n",
    "We will develop this by implementing a class for the agent. We will use a package called jdc, that will allow us to split the implementation of a class over several cells.\n",
    "\n",
    "We will be using torch for the implementation of the neural network and the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18836b84-bfcc-4d2e-a25c-cd1d9dbef9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jdc\n",
      "  Using cached jdc-0.0.9-py2.py3-none-any.whl.metadata (817 bytes)\n",
      "Using cached jdc-0.0.9-py2.py3-none-any.whl (2.1 kB)\n",
      "Installing collected packages: jdc\n",
      "Successfully installed jdc-0.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc\n",
    "import jdc\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d78c58-adcf-4645-9916-1a3316121f12",
   "metadata": {},
   "source": [
    "## Example: Cart Pole\n",
    "We will use an environment from OpenAI for this exercise. The goal is to balance a pole by moving the attached cart to the left or to the right. As the pole should be balanced as long as possible, the reward for each time step is +1. An episode is done when the angle of the pole becomes too large.\n",
    "\n",
    "The observation space gives some measurements about the pole, for example the angle. However, we actually do not need to know the specific details, as the neural network will just learn using the input.\n",
    "\n",
    "The actions are to move the cart to the left or to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6fccfa-3da9-4939-a71e-f1ae82ceb64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space: Discrete(2)\n",
      "Sample from the observation space: [ 2.8743753  -0.2951693  -0.06476364 -0.22680487]\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation space: {env.observation_space}')\n",
    "print(f'Action space: {env.action_space}')\n",
    "\n",
    "print(f'Sample from the observation space: {env.observation_space.sample()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b6bac-94a9-42ec-ad59-9a5bade325e3",
   "metadata": {},
   "source": [
    "The environment has a render function that we can use to display the state. The parameter 'render_mode' is used to specify the mode. For standalone applications, we can use 'human' that will open a window. Here we get the image as an array and display it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20961d56-4fe5-45d4-b67b-b791cb864706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR is invalid or not set in the environment.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc811ef-a89b-4dd0-b385-c199d8a535d5",
   "metadata": {},
   "source": [
    "We can also try to render a sequence of images. Depending on your browser the display might not be optimal...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780d4e76-70ea-45b4-9b90-205cf6dbeb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "# Test the function with a few steps\n",
    "env.reset()\n",
    "for _ in range(50):\n",
    "    env.step(env.action_space.sample())  # Take a random action\n",
    "    display_environment(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72df12-f213-4217-84fe-4b18b9bbdb94",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Our agent will use the class of the neural network (the model) as parameter, so that we can call it using different models. The first task is to write a example class that for the neural network. We will pass the number of observation values and the number of actions values as parameters.\n",
    "\n",
    "The model must calculate the q function for each state. We will also need to access those values for evaluation.\n",
    "\n",
    "Build a sequential model using at least two dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc9c401-f28d-4ed5-bfde-5990b8f3de02",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4f4206b89ef19b7d4ab280852a1f0bc",
     "grade": false,
     "grade_id": "cell-5f57f2a0435bd064",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, n_obs, n_action):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        # generate a sequential model in a internal variable (for example self.fc)\n",
    "        # YOUR CODE HERE\n",
    "        self.fc = nn.Sequential(nn.Linear(n_obs, 32), nn.ReLU(), nn.Linear(32, n_action))\n",
    "\n",
    "    def forward(self, x_tensor):\n",
    "        # forward should just call you model\n",
    "        # YOUR CODE HERE\n",
    "        return self.fc(x_tensor)\n",
    "        \n",
    "    def q_values(self, obs_tensor):\n",
    "        with torch.no_grad():\n",
    "            q = self.forward(obs_tensor)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9787f0e3-966e-4d85-94f8-e3950d45f90a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f9879b8c8b060c8ca5b8a646a2825b5",
     "grade": true,
     "grade_id": "cell-02f08792391930df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNetwork(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = DQNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "print(m)\n",
    "\n",
    "\n",
    "obs_sample = env.observation_space.sample()\n",
    "\n",
    "# models expect a batch of data, so we have to add a dimension and convert it to a tensor\n",
    "obs_batch = np.expand_dims(obs_sample, axis=0)\n",
    "obs_batch_tensor = torch.from_numpy(obs_batch).float()\n",
    "action_values = m.q_values(obs_batch_tensor)\n",
    "assert action_values.shape == (1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acfd08-b0fe-41e2-9ad3-47ef889cfa55",
   "metadata": {},
   "source": [
    "### Device managment in torch\n",
    "\n",
    "In order to use the gpu, we have to move our models to the device. We will also set this as\n",
    "default device, so that any tensors will be initialized on it and we don't have to specify them every time.\n",
    "\n",
    "In the code, we will use _tensor whenever a torch tensor is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ab168b-042e-403a-8e32-e5d2e98e721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # test if it worked\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using CUDA device')\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using MPS device')\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3cfd11-13eb-4fd3-a850-e3a0262446aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cd294-1491-4191-a25b-de8aafcd7762",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. Check the parameters and their descriptions as they will be used in the implementation and you will have to find suitable hyperparameters for them.\n",
    "\n",
    "\n",
    "The only thing to fill out is the optimizer and the loss function. \n",
    "\n",
    "There are different optimizers available, either standand SGD or Adam would be possible and should be initialized with the learning rate given in the parameters.\n",
    "\n",
    "What is the loss function that we have to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11fc0213-5dc9-4f23-ab2d-1d44d983a144",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc682665d090e45871c7275589f8d3fd",
     "grade": false,
     "grade_id": "cell-8c19c66d131c3fbd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space,\n",
    "                 model_cls,\n",
    "                 device,\n",
    "                 gamma: float,\n",
    "                 epsilon: float, epsilon_decay: float, epsilon_min: float,\n",
    "                 learning_rate: float, training_frequency: int, target_update_frequency: int,\n",
    "                 tau : float, use_double_dqn: bool,\n",
    "                 batch_size: int, memory_size: int):\n",
    "        \"\"\"\n",
    "        Initialise the agent.\n",
    "        Args:\n",
    "            observation_space: The observation space of the environment\n",
    "            action_space: The action space of the environment\n",
    "            model_cls: the class that implements the model,\n",
    "            device: the device to run torch on\n",
    "            gamma: The discount factor\n",
    "            epsilon: The initial epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: The decay factor for the epsilon value\n",
    "            epsilon_min: The minimal epsilon value after which it will not be decayed further\n",
    "            learning_rate: The learning rate for the optimizer\n",
    "            training_frequency: The frequency (in steps) of training the model\n",
    "            target_update_frequency: The frequency (in steps) of updating the target model (if not using tau)\n",
    "            tau: weight of the new model in the target update \n",
    "            use_double_dqn: use double q learning\n",
    "            batch_size: The batch size for training (sampled from the memory)\n",
    "            memory_size: The size of the memory for storing experiences\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "    \n",
    "        # hyperparameters from parameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.training_frequency = training_frequency\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "    \n",
    "        self.use_double_dqn = use_double_dqn\n",
    "        self.tau = tau\n",
    "\n",
    "        self.nr_training_steps = 0\n",
    "    \n",
    "        # internal variables\n",
    "        self.nr_steps = 0\n",
    "    \n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "\n",
    "        self.device = device\n",
    "    \n",
    "        # build the models\n",
    "        # self.model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "        # self.target_model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "        self.model = model_cls(self.observation_space.shape[0], self.action_space.n)\n",
    "        self.target_model = model_cls(self.observation_space.shape[0], self.action_space.n)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.target_model = self.target_model.to(self.device)\n",
    "\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.loss = None \n",
    "\n",
    "        # generate the optimizer and loss function in the variables above\n",
    "        # YOUR CODE HERE\n",
    "        self.optimizer = optim.Adam(self.model.parameters() ,lr=learning_rate)\n",
    "        self.loss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent to the initial state\n",
    "        \"\"\"\n",
    "        self.nr_steps = 0\n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "        self.nr_training_steps = 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cf988c-24c5-4b8e-ad91-e79fec78d7b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ee19ca899da78a8af841c665edc8caf",
     "grade": true,
     "grade_id": "cell-6638146a1a48950b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "q_agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       model_cls=DQNetwork,\n",
    "                       device=device,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.0,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe8a89-c401-40bc-a44e-4052b3a3d396",
   "metadata": {},
   "source": [
    "### Updating of the target model\n",
    "\n",
    "It is important in Q-Learning that the model that is trained and the target model used to calculate the target functions are not the same. The target model should stay fixed for a while or change only slowly.\n",
    "\n",
    "There are different methods to update the model:\n",
    "- Replace the target model every number of steps\n",
    "- Interpolate between the policy and the target model\n",
    "\n",
    "Implement the update to use the replacement when the parameter tau is equal to 0 and the interpolation (using tau) when tau is greater than 0. You should later experiment with both update functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7bcc187-fd5a-4689-ba07-f7e62d128f13",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e02fcb0302750b798ad2be8e7a21010",
     "grade": false,
     "grade_id": "cell-a1cb418924b7cb27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Update the target model with the weights from the current model. There are two possibilities for\n",
    "        updating:\n",
    "        - Replace the target model every number of steps\n",
    "        - Interpolate between the policy and the target model\n",
    "\n",
    "        In both cases the update_target_model method will be called every self.target_update_frequency \n",
    "        number of steps (not training steps). If the update is done by replacement, the value should be\n",
    "        higher, if the update is done by interpolation it can be lower.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        policy_net_state_dict = self.model.state_dict()\n",
    "\n",
    "        if self.tau > 0.0:\n",
    "            target_net_state_dict = self.target_model.state_dict()\n",
    "            # YOUR CODE HERE\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key] * self.tau + target_net_state_dict[key] * (1 - self.tau)\n",
    "            self.target_model.load_state_dict(target_net_state_dict)\n",
    "        else:\n",
    "            # YOUR CODE HERE\n",
    "            self.target_model.load_state_dict(policy_net_state_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbda48da-2108-4a2e-83a8-957b81d59023",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbcce6aa140ca971b00622c3915af067",
     "grade": true,
     "grade_id": "cell-cb441f633638dff8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "q_agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       model_cls=DQNetwork,\n",
    "                       device=device,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.0,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n",
    "q_agent.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b09c6-c2b2-45dc-81e2-c87179dab829",
   "metadata": {},
   "source": [
    "## Calculating actions\n",
    "\n",
    "Calculating actions is done with an epsilon greedy policy. However, for evaluation it is often suitable to use the greedy policy instead. So we add a parameter `stochastic`, if it is True then the epsilon-greedy policy is used, if not, the greedy policy is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "808c4b3c-82ec-456a-beb4-f94a3bc1e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def calculate_action(self, obs_tensor, stochastic: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the action for the given observation.\n",
    "    Args:\n",
    "        obs: the observation\n",
    "        stochastic: whether to use a stochastic (epsilon greedy) policy or not\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    if not stochastic or np.random.rand() > self.epsilon:\n",
    "        # calculate greedy action\n",
    "        with torch.no_grad():\n",
    "            action_value = self.model.q_values(obs_tensor)\n",
    "        return torch.argmax(action_value).numpy(force=True)\n",
    "    else:\n",
    "        # calculate random action\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d490aa0b-73f6-4001-8ee5-0387ff91a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 epsilon=0.9,\n",
    "                 epsilon_min=0.03,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 learning_rate=0.0005,\n",
    "                 training_frequency=1,\n",
    "                 target_update_frequency=2,\n",
    "                 tau=0.005,\n",
    "                 use_double_dqn=False,\n",
    "                 batch_size=256,\n",
    "                 memory_size=10000)\n",
    "obs_tensor = torch.tensor(env.observation_space.sample())\n",
    "a = agent.calculate_action(obs_tensor)\n",
    "assert env.action_space.contains(a)\n",
    "a = agent.calculate_action(obs_tensor, stochastic=False)\n",
    "assert env.action_space.contains(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cbec0-6b65-40b4-9bbf-c0c136afd366",
   "metadata": {},
   "source": [
    "## Add the steps\n",
    "\n",
    "Next we will add the two step functions as in the previous implementations of an agent. The `step_first` method is called after the environment is reset and we do not have any rewards yet.\n",
    "\n",
    "The `step` method is called for all other steps. In the step method we need to\n",
    "* Save the current experience (S, A, R, S', done) in the memory.\n",
    "* Calculate the next action\n",
    "* Save action and observation for next step\n",
    "* train the model every couple of steps\n",
    "* update the target model every couple of steps\n",
    "\n",
    "You have to fill in the code for the first three items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "922fa8c8-bc5d-4552-8709-91c6a8c9c421",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edc27e1ff54fbb99d45bdecba8e633e3",
     "grade": false,
     "grade_id": "cell-f11d5c8957197292",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def step_first(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action for the first step in the environment after a reset.\n",
    "    Args:\n",
    "        obs: The observation from the environment\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    # Um sicherzustellen, dass die Beobachtung als Tensor vorliegt\n",
    "    self.last_obs = np.array(obs)  # Konvertiere sicher in ein NumPy-Array\n",
    "    obs_tensor = torch.tensor(self.last_obs, dtype=torch.float32)  # Umwandlung in Tensor\n",
    "    \n",
    "    # Berechne die erste Aktion\n",
    "    self.last_action = self.calculate_action(obs_tensor)\n",
    "    return self.last_action\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "    \"\"\"\n",
    "    Der Agent führt eine Aktion aus, basierend auf der aktuellen Beobachtung, und speichert diese Erfahrung in der Memory.\n",
    "    Args:\n",
    "        obs: Die Beobachtung aus der Umgebung\n",
    "        reward: Der erhaltene Reward für die letzte Aktion\n",
    "        done: Ob die Episode abgeschlossen ist\n",
    "    Returns:\n",
    "        Die nächste Aktion\n",
    "    \"\"\"\n",
    "    # Konvertiere die Beobachtung in ein NumPy-Array und dann in einen Tensor\n",
    "    obs = np.array(obs)  # Um sicherzustellen, dass es ein Array ist\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32)  # Umwandlung in Tensor\n",
    "    \n",
    "    # Füge die letzte Erfahrung zum Replay-Speicher hinzu\n",
    "    self.memory.append((self.last_obs, self.last_action, reward, obs, done))\n",
    "\n",
    "    # Berechne die nächste Aktion\n",
    "    self.last_action = self.calculate_action(obs_tensor)\n",
    "    self.last_obs = obs  # Aktualisiere die letzte Beobachtung\n",
    "\n",
    "    self.nr_steps += 1\n",
    "\n",
    "    # Trainiere das Modell in festgelegtem Abstand\n",
    "    if self.nr_steps % self.training_frequency == 0:\n",
    "        self.train_model()\n",
    "\n",
    "    # Update das Zielmodell in festgelegtem Abstand\n",
    "    if self.nr_steps % self.target_update_frequency == 0:\n",
    "        self.update_target_model()\n",
    "\n",
    "    return self.last_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "679ad371-d595-4b86-9b6d-322f0d2c3434",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68f17895cc3cd32aa4caf076cf644041",
     "grade": true,
     "grade_id": "cell-55874102f12eeb67",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 epsilon=0.9,\n",
    "                 epsilon_min=0.03,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 learning_rate=0.0005,\n",
    "                 training_frequency=1,\n",
    "                 target_update_frequency=2,\n",
    "                 tau=0.005,\n",
    "                 use_double_dqn=False,\n",
    "                 batch_size=256,\n",
    "                 memory_size=10000)\n",
    "a = agent.step_first(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e1f9b-2b90-4d3f-94b7-adf575b31e33",
   "metadata": {},
   "source": [
    "## Train the model function\n",
    "\n",
    "The last thing to do is now to train the model. For this we have to \n",
    "* Sample from the memory to get a batch of observations, actions, rewards, next observations and dones\n",
    "* Calculate a better estimate for the q values of the current observation using q-learning\n",
    "* Fit the model to the updated values using gradient descend on the loss\n",
    "* Decay the epsilon value\n",
    "\n",
    "In torch, the gradient descend step has to be calculated in the code. Todo this:\n",
    "* Calculate the model output\n",
    "* Calculate the loss function\n",
    "* Clear the gradient (using self.optimizer.zero_grad())\n",
    "* Calculate a backwards step (resulting in the gradient)\n",
    "* Apply the step in the optimizer\n",
    "\n",
    "Note that between calculating the model, and calculating the loss function, you will have to calculate the target function (using the target_model), as no gradients are required on the target model, this code should be within a `with torch.no_grad()` block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3a5506a-0c18-4433-8b1d-529177bc2cf4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "642443bea50b66830d541a7458e57b1d",
     "grade": false,
     "grade_id": "cell-fe51b24157ad1aa1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train_model(self):\n",
    "    # not enough samples yet\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "\n",
    "    self.nr_training_steps += 1\n",
    "\n",
    "    # Sample random minibatch from memory\n",
    "    indices = np.random.choice(len(self.memory), size=self.batch_size, replace=False)\n",
    "\n",
    "    # Unpack batch\n",
    "    obs = np.array([self.memory[i][0] for i in indices])\n",
    "    actions = np.array([self.memory[i][1] for i in indices])\n",
    "    rewards = np.array([self.memory[i][2] for i in indices])\n",
    "    obs_next = np.array([self.memory[i][3] for i in indices])\n",
    "    dones = np.array([self.memory[i][4] for i in indices])\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(self.device)\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "    obs_next_tensor = torch.tensor(obs_next, dtype=torch.float32).to(self.device)\n",
    "    dones_tensor = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "    # Calculate Q-values for the current observation using the model\n",
    "    q_values = self.model(obs_tensor).gather(dim=1, index=actions_tensor)\n",
    "\n",
    "    # Calculate the target Q-values using the target model (without gradients)\n",
    "    with torch.no_grad():\n",
    "        next_q_values = self.target_model(obs_next_tensor).max(1, keepdim=True)[0]\n",
    "        target_q_values = rewards_tensor + (1 - dones_tensor) * self.gamma * next_q_values\n",
    "\n",
    "    # Compute the loss between current Q-values and target Q-values\n",
    "    loss = self.loss(q_values, target_q_values)\n",
    "\n",
    "    # Backpropagation to compute gradients\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # Decay epsilon\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53e246cf-b487-4e93-acc9-1b18e3d1417e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b4f1b16af9ee8053a8839b775cab2e9",
     "grade": true,
     "grade_id": "cell-38fb10ebd1e39242",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test with a small version of the agent\n",
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=20,\n",
    "                       tau=0.00,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=4,\n",
    "                       memory_size=16)\n",
    "obs, info = env.reset()\n",
    "action = agent.step_first(obs)\n",
    "\n",
    "for i in range(20):\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    action = agent.step(obs, reward, done)\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "        action = agent.step_first(obs)\n",
    "assert agent.nr_training_steps > 4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b96b12-e8dd-41f6-a8dc-6acd8c2f8223",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented a full DQN Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ef19c-2e2a-4d97-a173-34322b805f55",
   "metadata": {},
   "source": [
    "## Complete agent for training and evaluation.\n",
    "\n",
    "We will add some additional methods to the agent in order to train and evaluate it more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14b25676-7364-48ef-9ebb-9ac55e730bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self, env: gym.Env, nr_episodes_to_train: int,  eval_env: gym.Env, eval_frequency: int):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of steps.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_to_train: the number of episodes to train\n",
    "        eval_env: Environment for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step_first(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            done = done or truncated\n",
    "            a = self.step(obs, reward, done)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, 10)\n",
    "            print(f'Evaluation: episode {nr_episodes}, epsilon: {self.epsilon} mean reward: {np.mean(rewards)}')\n",
    "\n",
    "        if nr_episodes >= nr_episodes_to_train:\n",
    "            return\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = torch.from_numpy(obs).to(self.device)\n",
    "        a = self.calculate_action(obs_tensor, stochastic=False)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        # some environments do not support truncated episodes, so we additionally check for a maximal number of steps\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            obs_tensor = torch.from_numpy(obs).to(self.device)\n",
    "            a = self.calculate_action(obs_tensor, stochastic=False)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e050db9-1f7d-4290-9e6d-03c4a936fa69",
   "metadata": {},
   "source": [
    "## Example Training\n",
    "\n",
    "Here is an example with some hyperparameters and a short training time (that will not be enough to actually train the full agent). You can adjust the parameters and see if you get good results. But for handing in the exercise, put it back to a short training :-). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "732052e5-bc4f-40d4-a496-18babc377d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: episode 10, epsilon: 0.5 mean reward: 9.7\n",
      "Evaluation: episode 20, epsilon: 0.4935388573568574 mean reward: 9.2\n",
      "Evaluation: episode 30, epsilon: 0.4180155106735103 mean reward: 9.1\n",
      "Evaluation: episode 40, epsilon: 0.3612049370831954 mean reward: 9.0\n",
      "Evaluation: episode 50, epsilon: 0.32323834230492937 mean reward: 9.4\n",
      "Evaluation: episode 60, epsilon: 0.28897318999286387 mean reward: 9.1\n",
      "Evaluation: episode 70, epsilon: 0.2585989429107567 mean reward: 9.2\n",
      "Evaluation: episode 80, epsilon: 0.23351057019025656 mean reward: 9.1\n",
      "Evaluation: episode 90, epsilon: 0.20938461171088726 mean reward: 9.5\n",
      "Evaluation: episode 100, epsilon: 0.18850419724242717 mean reward: 9.4\n",
      "Evaluation: episode 110, epsilon: 0.1702161736853135 mean reward: 9.5\n",
      "Evaluation: episode 120, epsilon: 0.15324174453914366 mean reward: 10.0\n",
      "Evaluation: episode 130, epsilon: 0.13686022731194322 mean reward: 18.2\n",
      "Evaluation: episode 140, epsilon: 0.11533888365277938 mean reward: 15.5\n",
      "Evaluation: episode 150, epsilon: 0.10404495600612963 mean reward: 9.5\n",
      "Evaluation: episode 160, epsilon: 0.06252512057782335 mean reward: 95.3\n",
      "Evaluation: episode 170, epsilon: 0.030852419332555222 mean reward: 55.1\n",
      "Evaluation: episode 180, epsilon: 0.029970113248546433 mean reward: 132.6\n",
      "Evaluation: episode 190, epsilon: 0.029970113248546433 mean reward: 274.3\n",
      "Evaluation: episode 200, epsilon: 0.029970113248546433 mean reward: 308.9\n",
      "Evaluation: episode 210, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 220, epsilon: 0.029970113248546433 mean reward: 484.9\n",
      "Evaluation: episode 230, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 240, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 250, epsilon: 0.029970113248546433 mean reward: 487.3\n",
      "Evaluation: episode 260, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 270, epsilon: 0.029970113248546433 mean reward: 373.5\n",
      "Evaluation: episode 280, epsilon: 0.029970113248546433 mean reward: 359.6\n",
      "Evaluation: episode 290, epsilon: 0.029970113248546433 mean reward: 311.7\n",
      "Evaluation: episode 300, epsilon: 0.029970113248546433 mean reward: 277.0\n"
     ]
    }
   ],
   "source": [
    "env_train = gym.make(environment_name)\n",
    "env_eval = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    " # Hyperparameters will be quite important here.\n",
    "q_agent = DQNAgent(env_train.observation_space, env_train.action_space,\n",
    "                   model_cls=DQNetwork,\n",
    "                   device=device,\n",
    "                   gamma=0.99,\n",
    "                   epsilon=0.5,\n",
    "                   epsilon_min=0.03,\n",
    "                   epsilon_decay=0.999,\n",
    "                   learning_rate=0.0005,\n",
    "                   training_frequency=1,\n",
    "                   target_update_frequency=2,\n",
    "                   tau=0.01,\n",
    "                   use_double_dqn=False,\n",
    "                   batch_size=256,\n",
    "                   memory_size=10000)\n",
    "\n",
    "# the parameters are not optimal, but should already give a result\n",
    "q_agent.train(env_train, nr_episodes_to_train=300, eval_env=env_eval, eval_frequency=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe8b3c3b-1d54-4e5a-8a07-cef082a51899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[280.0, 266.0, 270.0, 283.0, 261.0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_agent.evaluate(env_eval, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fcb7ac82-ea78-4f37-bb3d-86cc843a1d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function with a few steps\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = torch.from_numpy(obs).to(device)\n",
    "for _ in range(200):\n",
    "    action = q_agent.calculate_action(obs_tensor, stochastic=False)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    obs_tensor = torch.from_numpy(obs).to(device)\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d4871-50fa-4122-97a5-134798d52b52",
   "metadata": {},
   "source": [
    "# Bonus Exercise\n",
    "\n",
    "Can you try training for another example, like the mountain car from the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f787162-82ef-4a51-8e8e-d488863dfa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "other_env = 'MountainCar-v0'\n",
    "\n",
    "env_train = gym.make(other_env)\n",
    "env_eval = gym.make(other_env, render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation space: {env_eval.observation_space}')\n",
    "print(f'Action space: {env_eval.action_space}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8aaaaa-c62e-4169-9dbd-8d8bcc9b0350",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4623ab4dec6a1df0eeb4f6c804f37f13",
     "grade": false,
     "grade_id": "cell-a4cde5d226be73e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69d7ad-d39a-454b-baed-3afb680f8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a few steps\n",
    "env = gym.make(other_env, render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = torch.from_numpy(obs).to(device)\n",
    "for _ in range(200):\n",
    "    action = q_agent.calculate_action(obs_tensor, stochastic=False)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    obs_tensor = torch.from_numpy(obs).to(device)\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e40e83-197e-4c0d-8915-77e241053ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
