{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "Temporal difference (TD) learning is a **key method** for RL. \n",
    "\n",
    "In comparison to MC methods, the update for any estimation is calculated at every time step and does not have to wait for the episode to finish.\n",
    "\n",
    "We will look at the prediction problem for the state-value function first and then go on to the control problem with SARSA and Q-learning, which will be implemented with the `Agent` interface from the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# progress bar, see https://github.com/tqdm/tqdm\n",
    "import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('.')\n",
    "from mdp import RState, MDPGridworld, WalledGridworld, SlipperyGridworld\n",
    "from mdp_env import MDPEnv, Agent\n",
    "from util import max_arg_with_ties, plot_policy_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same two gridworlds as in the last exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normal gridworld\n",
    "gw = MDPGridworld(height=4, width=3)\n",
    "gw.state(0, 0).is_terminal = True\n",
    "gw.state(3,2).color = 'lightgreen'\n",
    "\n",
    "# Slippery gridworld\n",
    "gws = SlipperyGridworld(4, 3)\n",
    "gws.state(0,0).is_terminal = True\n",
    "gws.state(3,2).color = 'lightgreen'\n",
    "\n",
    "gws.add_wall(0,0,1,0)\n",
    "gws.add_slippery_patch(1,2, 0.5)\n",
    "gws.add_slippery_patch(2,1, 0.5)\n",
    "\n",
    "# Plot the gridworlds and their ids (we will use id 11 as start, merked greed)\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "gw.render(gw.all_state_ids(), axs[0])\n",
    "gws.render(gws.all_state_ids(), axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the same env as in the last exercise. It is located and imported from a seperate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(0) prediction\n",
    "\n",
    "In the first exercise we want to implement the TD(0) algorithm to estimate the state-value function for a given policy. We will later use the action-values for control, but the state-values are slightly easier to calculate, so we will start with them.\n",
    "\n",
    "You should implement a constant step_size update, where the step_size is given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "953a7194ce5701dcfbb60a624ba8041f",
     "grade": false,
     "grade_id": "cell-c2a04a97ab52fb42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def td_prediction(env: gym.Env, nr_states: int, policy: np.ndarray, gamma: float, step_size: float, num_repetitions: int) \\\n",
    "        -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate the value function for the environment for the given policy\n",
    "\n",
    "    Args:\n",
    "        env: the environment\n",
    "        nr_states: the number of states in the MDP of the environment\n",
    "        policy: the policy to evaluate\n",
    "        gamma:\n",
    "        step_size: the step size for the value estimation update\n",
    "        num_repetitions: number of episodes for the prediction\n",
    "    \"\"\"\n",
    "    # initialize v\n",
    "    v = np.zeros(nr_states)\n",
    "    for _ in range(num_repetitions):\n",
    "\n",
    "        # initialize episode\n",
    "        state_id, _ = env.reset()\n",
    "\n",
    "        # take first action\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # take action and update v\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "946159c79f3f6a94f658ddcc59e22129",
     "grade": true,
     "grade_id": "cell-f50b22f6f8d4df0b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = MDPEnv(gw.states, gw.NR_ACTIONS, 11)\n",
    "p = np.zeros((len(gw.states), gw.NR_ACTIONS))\n",
    "p.fill(1.0 / gw.NR_ACTIONS)\n",
    "\n",
    "np.random.seed(1)\n",
    "v = td_prediction(env, len(gw.states), p, 1.0, 0.05, 1000)\n",
    "\n",
    "np.random.seed(1)\n",
    "v2 = td_prediction(env, len(gw.states), p, 0.9, 0.05, 1000)\n",
    "\n",
    "np.testing.assert_allclose(v, [  0., -16.14, -25.99,  -18.69,  -24.79, -29.25, -29.04, -32.23, -33.31, -34.83, -35.96, -37.11], atol=0.1)\n",
    "np.testing.assert_allclose(v2, [ 0., -4.83, -7.19, -5.25, -6.78, -7.73, -7.42, -8.07, -8.31, -8.42, -8.61, -8.79,], atol=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let us also look at the results\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "gw.render(v, axs[0])\n",
    "gw.render(v2, axs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## TD(0) on policy control: SARSA\n",
    "\n",
    "SARSA is the on-policy control algorithms for TD(0) with one step update and immediately using the new values for the policy (it is a bit similar to value iteration in Dynamic Programming). \n",
    "\n",
    "The policy must be epsilon-soft, so we will use an epsilon-greedy policy. In order to be able to calculate these policies we now need to estimate the action-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e35072dde42f4afdbac8782b51418b5",
     "grade": false,
     "grade_id": "cell-b95be6a2207ca2ac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SarsaAgent(Agent):\n",
    "    \"\"\"\n",
    "    Sarsa Agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space, gamma: float, step_size: float, epsilon: float):\n",
    "        \"\"\"\n",
    "        Initialize Sarsa\n",
    "\n",
    "        Args:\n",
    "            action_space: the action space\n",
    "            state_space: the state space\n",
    "            gamma: the discount factor\n",
    "            step_size: the step size for the update\n",
    "            epsilon: the epsilon for the epsilon greedy policy\n",
    "        \"\"\"\n",
    "        super().__init__(action_space, state_space)\n",
    "        self._env = env\n",
    "        self._epsilon = epsilon\n",
    "        self._gamma = gamma\n",
    "        self._step_size = step_size\n",
    "\n",
    "        # We want to estimate the state-action value function\n",
    "        self._q = np.zeros((state_space.n, self._action_space.n))\n",
    "\n",
    "        # we need to remember the last state and action\n",
    "        self._last_state_id = None\n",
    "        self._last_action = None\n",
    "\n",
    "    @property\n",
    "    def q(self):\n",
    "        return self._q\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the learning of the agent to an initial condition, so it can be used in a new env.\n",
    "        \"\"\"\n",
    "        self._q.fill(0.0)\n",
    "\n",
    "    def get_eps_greedy_action(self, state) -> int:\n",
    "        \"\"\"\n",
    "        Helper function to get the epsilon greedy action from the indicated state as we do\n",
    "        not need to specify a policy explicitely. Use max_arg_with_ties for the greedy case.\n",
    "\n",
    "        Return the taken action.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step_first(self, observation) -> int:\n",
    "        \"\"\"\n",
    "        First step after reset or initialization with the first observation from the environment. In our\n",
    "        gridwold environment the observation is the current state.\n",
    "        \"\"\"\n",
    "        # you will need to:\n",
    "        # - save the observation (the state)\n",
    "        # - determine the next action, save and return it\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, observation, reward, done) -> int or None:\n",
    "        \"\"\"\n",
    "        Step in the environment. The reward is the returned reward from the last action and the\n",
    "        observation contains the state after performing the last action.\n",
    "        Args:\n",
    "            observation: the observation from the environment (here the state_id)\n",
    "            reward: the reward from the last action\n",
    "            done: true if the episode is finished as a terminal state has been reached\n",
    "        Returns:\n",
    "            the action to take next or None if the episode is finished\n",
    "        \"\"\"\n",
    "        if not done:\n",
    "            # During the episodes you will need to\n",
    "            # - determine the next action\n",
    "            # - update the action-state estimates\n",
    "            # - save the state and action\n",
    "            # - return the action\n",
    "\n",
    "            # to make the calculation more clear\n",
    "            state_id = observation\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # episode is done, but we still need to calculate the last update,\n",
    "\n",
    "            # all action values from the terminal state are considered zero\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # we return None as there is no action from a terminal state\n",
    "            return None\n",
    "\n",
    "    def train(self, env, nr_episodes):\n",
    "        for _ in range(nr_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            a = self.step_first(obs)\n",
    "            done = False\n",
    "            while not done:\n",
    "                obs, reward, done, _, _ = env.step(a)\n",
    "                a = self.step(obs, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c84f520e65f61552e36662e270710e8",
     "grade": true,
     "grade_id": "cell-8d3225b21168c7a5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = MDPEnv(gw.states, gw.NR_ACTIONS, 11)\n",
    "agent = SarsaAgent(env.action_space, env.observation_space, gamma=1.0, step_size=0.05, epsilon=0.05)\n",
    "\n",
    "agent.reset()\n",
    "assert (agent.q == 0.0).all()\n",
    "\n",
    "# simulate one step\n",
    "a = agent.step_first(1)\n",
    "assert a >= 0\n",
    "assert a < 4\n",
    "\n",
    "# simulate end of episode\n",
    "a2 = agent.step(0, -1, True)\n",
    "assert a2 is None\n",
    "# this value was updated\n",
    "assert agent.q[1, a] == -0.05\n",
    "\n",
    "np.random.seed(1)\n",
    "agent.train(env, 10000)\n",
    "\n",
    "np.testing.assert_allclose([-2.03264807,-2.96864979,-3.1029341 ,-1.        ], agent.q[1,:], atol=1.0)\n",
    "np.testing.assert_allclose([-5.3443384, -6.16508002, -6.22824094, -5.21986845], agent.q[11,:], atol=1.0)\n",
    "\n",
    "# try with discount factor\n",
    "agent2 = SarsaAgent(env.action_space, env.observation_space, gamma=0.9, step_size=0.05, epsilon=0.05)\n",
    "agent2.train(env, 10000)\n",
    "np.testing.assert_allclose([-1.90006585, -2.65332789, -2.51679344, -1. ], agent2.q[1,:], atol=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "To evalulate the result we can plot the maximum value over the action in a state and the mean value over the actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "gw.render(agent.q.max(axis=1),axs[0])\n",
    "gw.render(agent2.q.max(axis=1),axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "We can see that even with discount factor 1.0, we do not get the optimal value for all states, for example the state (0,2) has the value -2.03, but -2 would be the value for the optimal policy. Why did we not get this? (Answer will be discussed in the lecture :-) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_from_action_values(q) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate a greedy policy from the action values.\n",
    "    \"\"\"\n",
    "    p = np.zeros(q.shape)\n",
    "    best_action = q.argmax(axis=1)\n",
    "    for a in range(q.shape[1]):\n",
    "        p[:, a] = (best_action == a).astype(float)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_greedy = greedy_from_action_values(agent.q)\n",
    "plot_policy_values(p_greedy, agent.q.max(axis=1), gw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Results of Sarsa\n",
    "\n",
    "We have to be careful with analyzing the plot, the printed value function corresponds to the greedy action which is shown in the array. But the state reached by that action might not be the one that displays the highest value. For example the q function at (1,2) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.q[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "\n",
    "The next agent that we want to implement is a q-learning agent. In comparison to SARSA is this an offline algorithms which will use the best of the available actions for the update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c95da7311293db49577828f68959c90d",
     "grade": false,
     "grade_id": "cell-aba5ff570ec2e713",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    \"\"\"\n",
    "    Q-Learning Agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space, gamma: float, step_size: float, epsilon: float):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            action_space: the action space\n",
    "            state_space: the state space\n",
    "            gamma: the discount factor\n",
    "            step_size: the step size for the update\n",
    "            epsilon: the epsilon for the epsilon greedy policy\n",
    "        \"\"\"\n",
    "        super().__init__(action_space, state_space)\n",
    "        self._env = env\n",
    "        self._epsilon = epsilon\n",
    "        self._gamma = gamma\n",
    "        self._step_size = step_size\n",
    "\n",
    "        # We want to estimate the state-action value function\n",
    "        self._q = np.zeros((state_space.n, self._action_space.n))\n",
    "\n",
    "        # we need to remember the last state and action\n",
    "        self._last_state_id = None\n",
    "        self._last_action = None\n",
    "\n",
    "    @property\n",
    "    def q(self):\n",
    "        return self._q\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the learning of the agent to an initial condition, so it can be used in a new env.\n",
    "        \"\"\"\n",
    "        self._q.fill(0.0)\n",
    "\n",
    "    def get_eps_greedy_action(self, state):\n",
    "        \"\"\"\n",
    "        Helper function to get the epsilon greedy action from the indicated state\n",
    "        \"\"\"\n",
    "\n",
    "        # this is the same as in SARSA\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step_first(self, observation) -> int:\n",
    "        \"\"\"\n",
    "        First step after reset or initialization with the first observation from the environment.\n",
    "        \"\"\"\n",
    "        # as there is no update, this is also the same as in SARSA\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, observation, reward, done) -> int or None:\n",
    "        \"\"\"\n",
    "        Step in the environment. The reward is the returned reward from the last action.\n",
    "        \"\"\"\n",
    "\n",
    "        if not done:\n",
    "            # to make the calculation more clear\n",
    "            state_id = observation\n",
    "\n",
    "            # you will have to do the same steps as in SARSA, but the update will be different\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # episode done, but we still need to calculate the last update, all action values\n",
    "            # from the terminal state are considered zero\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def train(self, env, nr_episodes):\n",
    "        for e in range(nr_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            a = self.step_first(obs)\n",
    "            done = False\n",
    "            while not done:\n",
    "                obs, reward, done, _, _ = env.step(a)\n",
    "                a = self.step(obs, reward, done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08cfbca279ee1326b627c7d4f117fd69",
     "grade": true,
     "grade_id": "cell-5e1af4a268bf0cd4",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = MDPEnv(gw.states, gw.NR_ACTIONS, 11)\n",
    "agent = QLearningAgent(env.action_space, env.observation_space, gamma=1.0, step_size=0.05, epsilon=0.05)\n",
    "\n",
    "agent.reset()\n",
    "assert (agent.q == 0.0).all()\n",
    "\n",
    "# simulate one step\n",
    "a = agent.step_first(1)\n",
    "assert a >= 0\n",
    "assert a < 4\n",
    "\n",
    "# simulate end of episode\n",
    "a2 = agent.step(0, -1, True)\n",
    "assert a2 is None\n",
    "# this value was updated\n",
    "assert agent.q[1, a] == -0.05\n",
    "\n",
    "agent.train(env, 10000)\n",
    "\n",
    "agent2 = QLearningAgent(env.action_space, env.observation_space, gamma=0.9, step_size=0.05, epsilon=0.05)\n",
    "agent2.train(env, 10000)\n",
    "\n",
    "np.testing.assert_allclose([-1.87121275, -2.68771325, -2.84147629, -1.        ], agent.q[1,:], atol=0.3)\n",
    "np.testing.assert_allclose([-1.80552557, -2.53051759, -2.48628421, -1.        ], agent2.q[1,:], atol=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "gw.render(agent.q.max(axis=1),axs[0])\n",
    "gw.render(agent2.q.max(axis=1),axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also plot the policy\n",
    "p_greedy = greedy_from_action_values(agent.q)\n",
    "plot_policy_values(p_greedy, agent.q.max(axis=1), gw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleepery Gridworld with SARSA and Q-Learning\n",
    "\n",
    "As a last example, let us compare SARSA and Q-Learning on the slippery gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MDPEnv(gws.states, gws.NR_ACTIONS, 11)\n",
    "agent_s = SarsaAgent(env.action_space, env.observation_space, gamma=1.0, step_size=0.05, epsilon=0.05)\n",
    "agent_q = QLearningAgent(env.action_space, env.observation_space, gamma=1.0, step_size=0.05, epsilon=0.05)\n",
    "\n",
    "np.random.seed(1)\n",
    "agent_s.train(env, 20000)\n",
    "agent_q.train(env, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "gws.render(agent_s.q.max(axis=1),axs[0])\n",
    "gws.render(agent_q.q.max(axis=1),axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_greedy_s = greedy_from_action_values(agent_s.q)\n",
    "plot_policy_values(p_greedy_s, agent_s.q.max(axis=1), gws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_greedy_q = greedy_from_action_values(agent_q.q)\n",
    "plot_policy_values(p_greedy_q, agent_q.q.max(axis=1), gws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the probability to fall in the slippery patches the algorithms will choose to go through the patches or avoid them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all for this exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
